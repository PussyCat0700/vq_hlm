{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import os\n",
    "import h5py\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "from constants import KEY_LM_HIDDEN_STATES, KEY_LM_INPUT_IDS, KEY_LM_LABELS\n",
    "from utils import load_config\n",
    "\n",
    "\n",
    "class HDF5Dataset(Dataset):\n",
    "    def __init__(self, h5_files_dir, split):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            h5_files_dir (str): \n",
    "            split (str): train/validation/test\n",
    "        \"\"\"\n",
    "        h5_file_path = os.path.join(h5_files_dir, split+'.h5')\n",
    "        self.h5_file = h5py.File(h5_file_path, 'r')\n",
    "        \n",
    "        self.hidden_states = self.h5_file[KEY_LM_HIDDEN_STATES]\n",
    "        self.input_ids = self.h5_file[KEY_LM_INPUT_IDS]\n",
    "        self.labels = self.h5_file[KEY_LM_LABELS]\n",
    "        self.total_samples = int(self.h5_file.attrs['total_samples'])\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.total_samples\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        input_ids = torch.tensor(self.input_ids[idx], dtype=torch.long)\n",
    "        labels = torch.tensor(self.labels[idx], dtype=torch.long)\n",
    "        hidden_states = torch.tensor(self.hidden_states[idx], dtype=torch.float)\n",
    "\n",
    "        return {\n",
    "            KEY_LM_INPUT_IDS: input_ids,\n",
    "            KEY_LM_LABELS: labels,\n",
    "            KEY_LM_HIDDEN_STATES: hidden_states\n",
    "        }\n",
    "\n",
    "    def close(self):\n",
    "        self.h5_file.close()\n",
    "\n",
    "\n",
    "class ChunkedHDF5Dataset(HDF5Dataset):\n",
    "    def __init__(self, h5_files_dir, split, chunk_size:int):\n",
    "        super().__init__(h5_files_dir, split)\n",
    "        self.chunk_size = chunk_size\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        item = super().__getitem__(idx)\n",
    "        hidden_states = item['hidden_states']\n",
    "        # last\n",
    "        # item['hidden_states'] = hidden_states[self.chunk_size-1::self.chunk_size, :]\n",
    "\n",
    "        # mean pooling\n",
    "        assert hidden_states.shape[0] % self.chunk_size == 0, \"行数不能被 chunk_size 整除\"\n",
    "\n",
    "        # 重塑张量为 (num_chunks, chunk_size, num_features)\n",
    "        hidden_states_reshaped = hidden_states.view(-1, self.chunk_size, hidden_states.shape[1])\n",
    "\n",
    "        # 对每个 chunk 计算均值，axis=1 表示按第二个维度（即每个块的行）计算均值\n",
    "        pooled_hidden_states = hidden_states_reshaped.mean(dim=1)\n",
    "\n",
    "        # 结果的形状是 (256, 768)\n",
    "        item['hidden_states'] = pooled_hidden_states\n",
    "        \n",
    "        \n",
    "        return item\n",
    "\n",
    "\n",
    "def get_chunked_h5dataloader(config_path, split):\n",
    "    config = load_config(config_path=config_path)\n",
    "    num_workers = 1  # Set num workers to 0 to enable debugging\n",
    "    shuffle = split == 'train'\n",
    "    dataset = ChunkedHDF5Dataset(config['h5_file_path'], split, chunk_size=config['chunk_size'])\n",
    "    # import pdb; pdb.set_trace()\n",
    "    dataloader = DataLoader(dataset, batch_size=config['batch_size'], shuffle=shuffle, num_workers=num_workers)\n",
    "    # import pdb; pdb.set_trace()\n",
    "    return dataloader\n",
    "\n",
    "dataloader = get_chunked_h5dataloader('conf/data/norm_layer10.yaml', 'test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def calculate_dataset_stats(dataloader, ratio=0.7):\n",
    "    total_loss = 0\n",
    "    # 遍历整个数据集并收集所有的 hidden_states\n",
    "    for i, batch in enumerate(dataloader):\n",
    "        hidden_states = batch[KEY_LM_HIDDEN_STATES]\n",
    "        mean = hidden_states.mean()\n",
    "        var = hidden_states.var()\n",
    "        # 创建一个形状为 (256,) 的掩码，标记出比例为 (1-ratio) 的部分\n",
    "        mask = torch.rand(hidden_states.size(1)) < ratio  # mask的大小是 (256,)\n",
    "        \n",
    "        # 扩展掩码到 (128, 256, 768) 形状\n",
    "        mask = mask.unsqueeze(0).unsqueeze(2).expand(hidden_states.size(0), hidden_states.size(1), hidden_states.size(2))\n",
    "        \n",
    "        # 生成一个形状与 hidden_states 相同的高斯分布张量\n",
    "        gaussian_fill = mean + torch.randn_like(hidden_states)*var  # 生成均值为0，方差为1的高斯分布\n",
    "        \n",
    "        # 将被掩盖为零的部分用高斯分布填充\n",
    "        masked_hidden_states = hidden_states * mask.float() + gaussian_fill * (1 - mask.float())\n",
    "        \n",
    "        # 计算重构损失\n",
    "        rec_loss = (masked_hidden_states - hidden_states).abs().mean()\n",
    "        total_loss += rec_loss.item()\n",
    "\n",
    "    total_loss /= len(dataloader)\n",
    "    return total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Loss for ratio 0.60: 0.05291\n",
      "Average Loss for ratio 0.62: 0.05001\n",
      "Average Loss for ratio 0.64: 0.04793\n",
      "Average Loss for ratio 0.66: 0.04343\n",
      "Average Loss for ratio 0.68: 0.03823\n",
      "Average Loss for ratio 0.70: 0.03833\n",
      "Average Loss for ratio 0.72: 0.03672\n",
      "Average Loss for ratio 0.74: 0.03375\n",
      "Average Loss for ratio 0.76: 0.03272\n",
      "Average Loss for ratio 0.78: 0.02834\n",
      "Average Loss for ratio 0.80: 0.02459\n",
      "Average Loss for ratio 0.82: 0.02221\n",
      "Average Loss for ratio 0.84: 0.01989\n",
      "Average Loss for ratio 0.86: 0.01952\n",
      "Average Loss for ratio 0.88: 0.01600\n",
      "Average Loss for ratio 0.90: 0.01327\n",
      "Average Loss for ratio 0.92: 0.00984\n",
      "Average Loss for ratio 0.94: 0.00751\n",
      "Average Loss for ratio 0.96: 0.00468\n",
      "Average Loss for ratio 0.98: 0.00284\n",
      "Average Loss for ratio 1.00: 0.00000\n"
     ]
    }
   ],
   "source": [
    "ratios = np.arange(0.6, 1.02, 0.02)  # 1.02 是为了确保包含 1.0\n",
    "\n",
    "for ratio in ratios:\n",
    "    losses = []  # 用于存储每个 ratio 下的 5 次损失值\n",
    "    \n",
    "    # 运行 5 次计算损失\n",
    "    for _ in range(5):\n",
    "        loss = calculate_dataset_stats(dataloader, ratio)\n",
    "        losses.append(loss)\n",
    "    \n",
    "    # 计算损失的均值\n",
    "    avg_loss = np.mean(losses)\n",
    "    \n",
    "    # 输出均值损失\n",
    "    print(f'Average Loss for ratio {ratio:.2f}: {avg_loss:.5f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([18, 256, 768])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
